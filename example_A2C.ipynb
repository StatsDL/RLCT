{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13c4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8786ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "from A2C.data_load import one_dimen_transform\n",
    "from A2C.A2C_train import A2Cagent\n",
    "from A2C.A2C_helper import Actor, Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb13784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "def coverage(intervals, target):\n",
    "\n",
    "    lower, upper = intervals[:, 0].reshape(-1,1), intervals[:, 1].reshape(-1,1)\n",
    "    horizon_coverages = np.logical_and(target >= lower, target <= upper)\n",
    "\n",
    "    return horizon_coverages, np.all(horizon_coverages, axis=1)\n",
    "\n",
    "def get_critical_scores(calibration_scores, q):\n",
    "\n",
    "    return np.transpose(np.array([\n",
    "        np.percentile(position_calibration_scores, q * 100)\n",
    "        for position_calibration_scores in calibration_scores\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3446c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLCT:\n",
    "    \n",
    "    def __init__(self, alpha, agent):\n",
    "    \n",
    "        self.agent = agent\n",
    "        self.alpha = alpha\n",
    "        self.calibration_scores = None\n",
    "        self.critical_calibration_scores = None\n",
    "        self.corrected_critical_calibration_scores = None\n",
    "        \n",
    "    def nonconformity(self, output, action):\n",
    "        return np.abs(output - action)\n",
    "    \n",
    "    \n",
    "    def fit(self, s, dataset_name, states_train, train_y, states_cal, cal_y, states_test, test_y, MAX_EPISODES, action_low, action_high):\n",
    "\n",
    "        Reward = []\n",
    "        Final_Score = []\n",
    "        initial_mse = np.inf\n",
    "        \n",
    "        for episode  in range(MAX_EPISODES):\n",
    "            index = np.random.choice(range(len(states_train)))\n",
    "            ep_reward = 0\n",
    "            self.agent.agent_mode('train')\n",
    "            for step in range(index, states_train.shape[0]-1): \n",
    "                # state_tensor = torch.FloatTensor(states_train[step]).reshape(1,-1).to(device)\n",
    "                state = states_train[step]\n",
    "                action, logs_probs = self.agent.get_action(state)\n",
    "                action = action.cpu()\n",
    "                action = action.detach().numpy()\n",
    "                logs_probs = logs_probs.cpu()\n",
    "                logs_probs = logs_probs.detach().numpy()\n",
    "                \n",
    "                reward = -abs(action-train_y[step])   \n",
    "                ep_reward += reward \n",
    "                        \n",
    "                next_state = states_train[step+1]\n",
    "                self.agent.update(state, action, reward, logs_probs, next_state)\n",
    "                \n",
    "            print('Episode %d : %.2f'%(episode+1,ep_reward))\n",
    "            Reward.append(ep_reward)\n",
    "            \n",
    "            self.agent.agent_mode('eval')\n",
    "            Test_actions = self.agent.regression_prediction(states_test)\n",
    "\n",
    "            Test_actions = torch.reshape(torch.tensor(Test_actions), (test_y.shape[0],1))\n",
    "            final_result = self.agent.critic_criterion(Test_actions, torch.tensor(test_y))\n",
    "            print('Episode %d final result: %.2f'%(episode+1,final_result))\n",
    "            \n",
    "            if final_result < initial_mse:\n",
    "                model_states = {\n",
    "                                'actor': self.agent.actor.state_dict(),\n",
    "                                'critic': self.agent.critic.state_dict()}\n",
    "                torch.save(model_states, f'{dataset_name}/A2C_best_agent.pth')\n",
    "                initial_mse = final_result\n",
    "                \n",
    "        \n",
    "        model_states = torch.load(f'A2C\\\\{dataset_name}\\\\A2C_best_agent_{s}.pth')\n",
    "\n",
    "        self.agent.actor.load_state_dict(model_states['actor'])\n",
    "        self.agent.critic.load_state_dict(model_states['critic'])\n",
    "        \n",
    "        self.calibrate(states_cal, cal_y)\n",
    "        \n",
    "    def calibrate(self, states_cal, cal_y):\n",
    "        \n",
    "        self.agent.agent_mode('eval')\n",
    "        cal_actions = self.agent.regression_prediction(states_cal)\n",
    "        cal_actions = torch.reshape(torch.tensor(cal_actions), (cal_y.shape[0],1))\n",
    "        \n",
    "        score = self.nonconformity(cal_actions, cal_y)\n",
    "        self.calibration_scores = np.transpose(np.array(score))\n",
    "        \n",
    "        \n",
    "        q = min(((len(cal_y) + 1.0) * (1 - self.alpha) / len(cal_y)), 1)\n",
    "        self.critical_calibration_scores = get_critical_scores(calibration_scores= self.calibration_scores, q=q)\n",
    "        \n",
    "    \n",
    "    def predict(self, states):\n",
    "        \n",
    "        self.agent.agent_mode('eval')\n",
    "        out = self.agent.regression_prediction(states)\n",
    "        out = torch.tensor(out).reshape(-1,1)\n",
    "        \n",
    "        lower = out - self.critical_calibration_scores\n",
    "        upper = out + self.critical_calibration_scores\n",
    "        \n",
    "        \n",
    "        return np.hstack((lower.reshape(-1, 1), upper.reshape(-1, 1)))\n",
    "    \n",
    "    def evaluate_coverage(self, test_states, test_actions):\n",
    "    \n",
    "        pred_intervals = self.predict(test_states)\n",
    "        independent_coverages, joint_coverages = coverage(pred_intervals, test_actions)\n",
    "\n",
    "        return independent_coverages, joint_coverages, pred_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba3d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: EURUSD\n",
      "\n",
      "\n",
      "Coverage (1):0.8340425491333008\n",
      "Interval Width (1):0.05232474952385647\n",
      "Coverage (2):0.957446813583374\n",
      "Interval Width (2):0.05174005724128971\n",
      "Coverage (3):0.9446808695793152\n",
      "Interval Width (3):0.051547974933680574\n",
      "Coverage (4):0.9617021083831787\n",
      "Interval Width (4):0.058774540933285484\n",
      "Coverage (5):0.9489361643791199\n",
      "Interval Width (5):0.05288120711548728\n",
      "\n",
      "\n",
      "Coverage:0.9293617010116577 $\\pm$ 0.04803800955414772\n",
      "Interval Width:0.053453705949519904 $\\pm$ 0.0027011843717314136\n",
      "\n",
      "\n",
      "\n",
      "Dataset: AUDUSD\n",
      "\n",
      "\n",
      "Coverage (1):0.957446813583374\n",
      "Interval Width (1):0.06017639681018149\n",
      "Coverage (2):0.957446813583374\n",
      "Interval Width (2):0.06110608360794778\n",
      "Coverage (3):0.8723404407501221\n",
      "Interval Width (3):0.0633848711993904\n",
      "Coverage (4):0.957446813583374\n",
      "Interval Width (4):0.059441865151830135\n",
      "Coverage (5):0.8765957355499268\n",
      "Interval Width (5):0.05461731977166329\n",
      "\n",
      "\n",
      "Coverage:0.92425537109375 $\\pm$ 0.04067337140440941\n",
      "Interval Width:0.05974530730820262 $\\pm$ 0.0028869467931317725\n",
      "\n",
      "\n",
      "\n",
      "Dataset: GBPUSD\n",
      "\n",
      "\n",
      "Coverage (1):0.957446813583374\n",
      "Interval Width (1):0.10399387207733456\n",
      "Coverage (2):0.9744681119918823\n",
      "Interval Width (2):0.10765505093494697\n",
      "Coverage (3):0.9744681119918823\n",
      "Interval Width (3):0.12039908109957943\n",
      "Coverage (4):0.8851063847541809\n",
      "Interval Width (4):0.09231424731170135\n",
      "Coverage (5):0.9489361643791199\n",
      "Interval Width (5):0.09413450564432169\n",
      "\n",
      "\n",
      "Coverage:0.9480851292610168 $\\pm$ 0.0330054871737957\n",
      "Interval Width:0.1036993514135768 $\\pm$ 0.010156004556029746\n",
      "\n",
      "\n",
      "\n",
      "Dataset: CNYUSD\n",
      "\n",
      "\n",
      "Coverage (1):0.978723406791687\n",
      "Interval Width (1):0.5460535749661685\n",
      "Coverage (2):0.9744681119918823\n",
      "Interval Width (2):0.5230471949812676\n",
      "Coverage (3):0.978723406791687\n",
      "Interval Width (3):0.5460535749661685\n",
      "Coverage (4):0.9744681119918823\n",
      "Interval Width (4):0.5230471949812676\n",
      "Coverage (5):0.978723406791687\n",
      "Interval Width (5):0.5356863172818126\n",
      "\n",
      "\n",
      "Coverage:0.977021336555481 $\\pm$ 0.0020846601109951735\n",
      "Interval Width:0.534777571435337 $\\pm$ 0.01029879404373643\n",
      "\n",
      "\n",
      "\n",
      "Dataset: CADUSD\n",
      "\n",
      "\n",
      "Coverage (1):0.957446813583374\n",
      "Interval Width (1):0.06830746502191977\n",
      "Coverage (2):0.9744681119918823\n",
      "Interval Width (2):0.06872999995980257\n",
      "Coverage (3):0.9659574627876282\n",
      "Interval Width (3):0.06712929725133199\n",
      "Coverage (4):0.9659574627876282\n",
      "Interval Width (4):0.06478844215857515\n",
      "Coverage (5):0.9531915187835693\n",
      "Interval Width (5):0.0702001291288532\n",
      "\n",
      "\n",
      "Coverage:0.9634042978286743 $\\pm$ 0.007419403642416\n",
      "Interval Width:0.06783106670409653 $\\pm$ 0.0018109410602046472\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['EURUSD', 'AUDUSD', 'GBPUSD', 'CNYUSD', 'CADUSD']\n",
    "for dataset_name in datasets:\n",
    "    covp, iw = [], []\n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    data_dir = f'datasets/{dataset_name}.csv'\n",
    "    # Reading the data\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(data_dir, index_col='Date')\n",
    "    df.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis=1, inplace=True)\n",
    "    \n",
    "    # print(df)\n",
    "    # length of time series data\n",
    "    L = int(df.shape[0])\n",
    "    \n",
    "    # length of train and validation data\n",
    "    len_train = int(math.ceil(L * .8))\n",
    "    len_test = int(math.ceil(L * .1))\n",
    "    len_val = L - len_train - len_test\n",
    "    \n",
    "    # splitting the original time series into train, validation, and test data\n",
    "    df_train = df[0: len_train]\n",
    "    df_val = df[len_train: len_train + len_val]\n",
    "    df_test = df[len_train + len_val: L]\n",
    "    \n",
    "    # train, validation, and test data values using their DataFrames\n",
    "    train_data = df_train.values\n",
    "    val_data = df_val.values\n",
    "    test_data = df_test.values\n",
    "    \n",
    "    \n",
    "    # ###############################################################################\n",
    "    # look_back is the time-horizon taken to predict one-day ahead prediction\n",
    "    look_back = 20\n",
    "    \n",
    "    states_train, states_val, train_y, val_y, _, _ = one_dimen_transform(train_data, val_data, look_back)\n",
    "    _, states_test, _, test_y, _, test_scaler = one_dimen_transform(val_data, test_data, look_back)\n",
    "\n",
    "    #####################  hyper parameters  ####################\n",
    "    action_low = 0.0\n",
    "    action_high = 1.0   \n",
    "    MAX_EPISODES = 500\n",
    "    #############################################################\n",
    "    \n",
    "    state_dim = states_train.shape[1]\n",
    "    action_dim = 1\n",
    "\n",
    "    with open(os.getcwd() + f'\\\\A2C\\\\{dataset_name}\\\\A2C_best_params.pkl', 'rb') as file:\n",
    "      best = pickle.load(file)\n",
    "      \n",
    "    agent = A2Cagent(state_dim, action_dim, best)\n",
    "    print(\"\\n\")\n",
    "    for s in range(1, 6):\n",
    "        model = RLCT(alpha = 0.1, agent = agent)\n",
    "        model.fit(s, dataset_name, states_train, train_y, states_val, val_y, states_test, test_y, MAX_EPISODES, action_low, action_high)\n",
    "        independent_coverages, joint_coverages, intervals = model.evaluate_coverage(states_test, test_y)\n",
    "        \n",
    "            \n",
    "        mean_independent_coverage = np.mean(np.array(independent_coverages, dtype=np.float32), axis=0)\n",
    "        interval_width = np.mean(test_scaler.inverse_transform(intervals[:, 1].reshape(-1,1)) - test_scaler.inverse_transform(intervals[:, 0].reshape(-1,1)), axis=0)\n",
    "        \n",
    "        \n",
    "        covp.append(mean_independent_coverage)\n",
    "        iw.append(interval_width)\n",
    "        \n",
    "        \n",
    "        print(f\"Coverage ({s}):{mean_independent_coverage.item()}\")\n",
    "        print(f\"Interval Width ({s}):{interval_width.item()}\")\n",
    "    \n",
    "    with open(f'A2C\\\\{dataset_name}\\\\coverage_prob_mean.pkl', 'wb') as file: pickle.dump(np.mean(covp), file)\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\coverage_prob_std.pkl', 'wb') as file: pickle.dump(np.std(covp), file)\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\interval_width_mean.pkl', 'wb') as file: pickle.dump(np.mean(iw), file)\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\interval_width_std.pkl', 'wb') as file: pickle.dump(np.std(iw), file)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\coverage_prob_mean.pkl', 'rb') as file:\n",
    "        covp_mean = pickle.load(file)\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\coverage_prob_std.pkl', 'rb') as file:\n",
    "        covp_std = pickle.load(file)\n",
    "\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\interval_width_mean.pkl', 'rb') as file:\n",
    "        iw_mean = pickle.load(file)\n",
    "    with open(f'A2C\\\\{dataset_name}\\\\interval_width_std.pkl', 'rb') as file:\n",
    "        iw_std = pickle.load(file)\n",
    "        \n",
    "    print(f\"Coverage:{covp_mean} $\\pm$ {covp_std}\")\n",
    "    print(f\"Interval Width:{iw_mean} $\\pm$ {iw_std}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
